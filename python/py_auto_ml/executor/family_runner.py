import json
from pyspark.sql.functions import DataFrame
from py_auto_ml.local_spark_singleton import SparkSingleton
from py_auto_ml.utilities.helpers import Helpers


class FamilyRunner:
    def __init__(self):
        self.spark = SparkSingleton.get_instance()

    def run_family_runner(self,
                          df: DataFrame,
                          prediction_type: str,
                          family_configs: dict):
        """

        :param family_configs: dict
            Supported model_family as a key, vaue is a dictionary of configuration overrides
        :param prediction_type: str
            "regressor" or "classifier"
        :param df: dataframe
        :param path: string
            Path to writing to writing out the pipeline models
        :return:
        """
        # Checking for supported model families and types
        Helpers.check_model_family(model_family)
        Helpers.check_prediction_type(prediction_type)

        stringified_family_configs = json.dumps(family_configs)
        self.spark._jvm.com.databricks.labs.automl.pyspark.FamilyRunnerUtil.runFamilyRunner(df._jdf,
                                                                                            stringified_family_configs,
                                                                                            prediction_type)

        self._family_runner = True

        return self._get_returns()

    # Fetch  the temp tables and bring them into python

    def _get_returns(self):
        """

        :return: dict of dataframes
            'model_report':model_report: dataframe
            'generation_report':generation_report: dataframe
            'best_mlflow_run_id':best_mlflow_run_id: dataframe
        """
        if self._family_runner != True:
            raise Exception("You must first run the family runner to generate the proper return dataframes")
        else:
            model_report = self.spark.sql("SELECT * FROM modelReportDataFrame")
            generation_report = self.spark.sql("SELECT * FROM generationReportDataFrame")
            best_mlflow_run_id = self.spark.sql("SELECT * FROM bestMlFlowRunId")
            return_dict = {
                'model_report': model_report,
                'generation_report': generation_report,
                'best_mlflow_run_id': best_mlflow_run_id
            }
            return return_dict

    # Get the best mlflow run Id from DF
    # Get pipeline path from tag in MLflow using tracking client
    # Returns pipeline model

    # TO DO full inference on pipeline model
    def mlflow_pipeline_inference(self,
                                  mlflow_run_id: str,
                                  model_family: str,
                                  prediction_type: str,
                                  dataframe: DataFrame,
                                  configs = [],
                                  label_col='label'):
        """

        :param mlflow_run_id: string
            The Mflow Run Id (as generated by AutoML) which has the pipeline model of interest
        :param model_family: string
            Support model family
        :parameter prediction_type: string
            Supported prediction type
        :param dataframe
            The dataframe being used for inference
        :param configs
            Dictionary of configuration overrides, default is empty dictionary
        :param label_col: string
            Label column of dataset that will be used for inference. Default is "label"
        :return inferred_df: dataframe
            The dataframe with prediction
        """
        # Checking for supported model families and types
        Helpers.check_model_family(model_family)
        Helpers.check_prediction_type(prediction_type)

        stringified_configs = json.dumps(configs)

        self.spark._jvm.com.databricks.labs.automl.pyspark.FamilyRunnerUtil.runMlFlowInference(mlflow_run_id,
                                                                                         model_family,
                                                                                         prediction_type,
                                                                                         label_col,
                                                                                         stringified_configs,
                                                                                         dataframe._jdf)
        # Pull out the inference
        inference_df = self.spark.sql("SELECT * FROM inferenceDF")

        return inference_df

    def path_pipeline_inference(self,
                           path: str,
                           dataframe: DataFrame):
        """

        :param path:
            Path to the pipelined model created by AutoML
        :param dataframe:
            Spark dataframe that will be used for inference
        :return: inference_df: Dataframe
            Dataframe with predictions
        """
        # Checking for supported model families and types
        Helpers.check_model_family(model_family)
        Helpers.check_prediction_type(prediction_type)

        self.spark._jvm.com.databricks.labs.automl.pyspark.FamilyRunnerUtil.runPathInference(path,
                                                                                             dataframe._jdf)
        inferred_df = self.spark.sql('SELECT * FROM pathInferenceDF')

        return inferred_df

    def feature_eng_pipeline(self,
                             df: DataFrame,
                             model_family: str,
                             prediction_type: str,
                             configs= {}
                             ):
        """

        :param df: Dataframe
            Dataframe feature engineering pipeline will be applied to
        :param model_family: string
            Supported model family
        :param prediction_type: string
            Supported prediction type
        :param configs: dict
            Dictionary of overrides
        :return: Dataframe
            Feature engineered dataframe
        """

        # Checking for supported model families and types
        Helpers.check_model_family(model_family)
        Helpers.check_prediction_type(prediction_type)


        stringified_family_configs = json.dumps(configs)
        self.spark._jvm.com.databricks.labs.automl.pyspark.FamilyRunnerUtil.runFeatureEngPipeline(df._jdf,
                                                                                                  model_family,
                                                                                                  prediction_type,
                                                                                                  stringified_family_configs)
        feature_eng_df = self.spark.sql("SELECT * FROM featEngDf")

        return feature_eng_df








