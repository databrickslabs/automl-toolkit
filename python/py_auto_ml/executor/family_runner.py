import json
from pyspark.sql.functions import DataFrame
from python.py_auto_ml.local_spark_singleton import SparkSingleton


class FamilyRunner:
    def __init__(self,
                 # family_configs: dict,
                 # prediction_type: str,
                 # df: DataFrame
                 ):
        self.spark = SparkSingleton.get_instance()
        # self.run_family_runner(family_configs,
        #                        prediction_type,
        #                        df)
        # self._get_returns()

    def run_family_runner(self,
                          family_configs: dict,
                          prediction_type: str,
                          df: DataFrame):
        """

        :param family_configs: dict
            Supported model_family as a key, vaue is a dictionary of configuration overrides
        :param prediction_type: str
            "regressor" or "classifier"
        :param df: dataframe
        :return:
        """
        stringified_family_configs = json.dumps(family_configs)
        self.spark._jvm.com.databricks.labs.automl.pyspark.FamilyRunnerUtil.runFamilyRunner(stringified_family_configs,
                                                                                            prediction_type,
                                                                                            df._jdf)
        self._family_runner = True

        self._get_returns()

    # Fetch  the temp tables and bring them into python

    def _get_returns(self):
        """

        :return: model_report: dataframe
            generation_report: dataframe
            best_mlflow_run_id: dataframe
        """
        if self._family_runner != True:
            raise Exception("You must first run the family runner to generate the proper return dataframes")
        else:
            self.model_report = self.spark.sql("SELECT * FROM modelReportDataFrame")
            self.generation_report = self.spark.sql("SELECT * FROM generationReportDataFrame")
            self.best_mlflow_run_id = self.spark.sql("SELECT * FROM bestMlFlowRunId")

    # Get the best mlflow run Id from DF
    # Get pipeline path from tag in MLflow using tracking client
    # Returns pipeline model

    # TO DO full inference on pipeline model
    def pipeline_inference(self,
                           mlflow_run_id: str,
                           model_family: str,
                           prediction_type:str,
                           dataframe: DataFrame,
                           configs = None,
                           label_col='label'):
        """

        :param mlflow_run_id: string
            The Mflow Run Id (as generated by AutoML) which has the pipeline model of interest
        :param model_family: string
            Support model family
        :param df
            The dataframe being used for inference
        :param label_col: string
            Label column of dataset that will be used for inference. Default is "label"
        :return inferred_df: dataframe
            The dataframe with prediction
        """
        stringified_configs = json.dumps(configs)
        self.spark._jvm.com.databricks.labs.automl.pyspark.FamilyRunnerUtil.runFamilyRunner(mlflow_run_id,
                                                                                            model_family,
                                                                                            prediction_type,
                                                                                            label_col,
                                                                                            stringified_configs,
                                                                                            dataframe)
        self.inference_df = self.spark.sq("SELECT * FROM inferenceDF")





